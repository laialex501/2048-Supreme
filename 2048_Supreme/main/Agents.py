"""Q-Learning Agents Here"""

import Util
from Game_World import *
import time
import os
import traceback
import sys
import random

class Agent: 

    def __init__(self, index=0):
        """Index gives agent a unique identifier"""
        self.index = index

    def getAction(self):
        """Agent will receive a gameState and must return an action to follow."""
        raiseNotDefined()

class ReinforcementAgent(Agent):
    """
    Agent which trains using approximate Q-values generated by weights and feature functions.
    """

    def __init__(self, alpha=0.5, epsilon=0.05, gamma=0.8, maxEpisodes=10, features=[], rewardFunc=lambda: 0, game=Game_2048):
        """
        Sets options for a reinforcement learning agent.

        Attributes:
        -----------
        alpha: the learning rate of the agent
        epsilon: the probability of taking a random action
        gamma: the discount factor on future rewards
        features: the feature functions used by this agent
        weights: the weights on the feature functions, sharing indices
        maxEpisodes: the number of episodes this agent will be trained on
        numEpisodes: the number of episodes this agent has seen so far
        rewardFunc: a function that gives you the reward for a transition
        game: the game that this agent will train on
        """
        self.alpha = float(alpha)
        self.epsilon = float(epsilon)
        self.gamma = float(gamma)
        self.features = features
        self.weights = []
        for feature in features:
            self.weights.append(0)
        self.maxEpisodes = maxEpisodes
        self.numEpisodes = 0
        self.rewardFunc = rewardFunc
        self.GAME = game

    def runEpisode(self, game):
        """
        Orders the agent to train by taking actions in the given game.
        """
        while True:
            startState = game.get_current_state()
            action = self.getAction(startState)
            if action != None:
                nextState = game.takeAction(action)
                reward = self.rewardFunc(startState, action, nextState)
                self.observeTransition(startState, action, nextState, reward)
            else:
                # If we are in a terminal state, then we conclude the episode
                self.finishEpisode()
                break

    def startNewGame(self):
        """
        Starts a new game for our agent and returns it
        """
        return self.GAME()

    def calculateMaxQValue(self, state):
        """
        Takes in a state and calculates the maximum q-value we can achieve
        at that state, and returns it.

        Will approximate the q-value with weights and feature functions
        """
        max_action = self.getMaxAction(state)
        if max_action == None:
            return 0
        max_q_value = self.calculateQValue(state, max_action)
        return max_q_value

    def calculateQValue(self, state, action):
        """
        Calculates the approximate Q-value of a (state, action) pair using
        our weights and feature functions.

        That is, Q(s, a) = w_1 * f_1(s, a) + ... + w_n * f_n(s, a)

        Inputs:
        -------
        state: our starting state
        action: an action we take at that state

        Outputs:
        --------
        q_value: a floating number representing our estimated q-value
        """
        q_value = 0
        for index in range(len(self.features)):
            feature = self.features[index]
            q_value += feature(state, action)
        return q_value

    def getLegalActions(self, state):
        """
        Finds the legal actions at a given state
        """
        return self.GAME().get_legal_actions(state)

    def getAction(self, state):
        """
        The approximate Q-learning agent will return an action to take at a given state.

        If in training, there is a self.epsilon probability of taking a random action. 
        Otherwise we take the best action we can see at the state. 

        If not in training, self.epsilon = 0 such that we always take the best action.
        """
        takeRandomAction = Util.flipCoin(self.epsilon)
        if takeRandomAction:
            actions = self.getLegalActions(state)
            # if there are no actions, then nothing you can do
            if actions == []:
                return None
            return random.choice(actions)
        else:
            return self.getMaxAction(state)

    def getMaxAction(self, state):
        """
        Returns the action with maximum Q-value at a given state
        """
        actions = self.getLegalActions(state)
        # if there are no actions, then nothing you can do
        if actions == []:
            return None
        best_action = actions[0]
        best_q_value = self.calculateQValue(state, best_action)
        for action in actions:
            q_value = self.calculateQValue(state, action)
            if q_value > best_q_value:
                best_action = action
                best_q_value = q_value
        return best_action

    def observeTransition(self, startState, action, nextState, reward):
        """
        Observes a transition from one state to the next. If we are in training,
        then it will also update our weights based on the transition. 

        Inputs:
        -------
        startState: our starting state
        action: the action we took at startState
        nextState: the state we reached after taking the action
        reward: the reward we received for our transition
        """
        if self.inTraining():
            q_sample = reward + self.gamma * self.calculateMaxQValue(nextState)
            diff = q_sample - self.calculateQValue(startState, action)
            for index in range(len(self.features)):
                feature = self.features[index]
                self.weights[index] = self.weights[index] + self.alpha * diff * feature(startState, action)
        """ What do you do if it's a terminal state? """

    def finishEpisode(self):
        """
        Completes an episode
        """
        self.numEpisodes += 1

    def adjustAlpha(self, new_alpha):
        """
        Adjusts our alpha to a new value
        """
        self.alpha = new_alpha

    def adjustEpsilon(self, new_epsilon):
        """
        Adjusts our epsilon to a new value
        """
        self.epsilon = new_epsilon

    def inTraining(self):
        """
        Returns true if we are still in training, false if we have finished
        """
        return (self.numEpisodes < self.maxEpisodes)